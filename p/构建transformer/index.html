<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="Transformer学习记录，py3.8\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 import torch import torch.nn as nn import torch.nn.functional as F import math # 不带可训练权重的PositionalEncoding class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 &#34;&#34;&#34; super(PositionalEncoding, self).__init__() # 初始化位置编码矩阵，形状为(max_len=5000, d_model=1000)，其中的元素都是0。 self.encoding = torch.zeros(max_len, d_model) # 生成位置信息，形状为(max_len=5000, 1)，其中的元素是从0到max_len-1的整数。 position = torch.arange(0, max_len).unsqueeze(1).float() # # 计算分母的指数项，用于生成正弦和余弦函数的周期。 # # torch.arange(0, d_model, 2).float()：创建了一个从0到d_model-1的整数序列，步长为2 # div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) # # 利用正弦和余弦函数生成位置编码矩阵 # self.encoding[:, 0::2] = torch.sin(position * div_term) # self.encoding[:, 1::2] = torch.cos(position * div_term) # 利用正弦和余弦函数生成位置编码矩阵 div_term = torch.arange(0, d_model, 2).float() / d_model self.encoding[:, 0::2] = torch.sin(position / torch.pow(10000.0, div_term)) self.encoding[:, 1::2] = torch.cos(position / torch.pow(10000.0, div_term)) # 将位置编码矩阵扩展为三维 # self.encoding 是一个形状为 (max_len, d_model) 的二维张量，表示位置编码矩阵 # 通过 unsqueeze(0)，形状变为 (1, max_len, d_model) # 以便与输入嵌入的词向量(batch_size, sequence_length, embedding_dim)进行相加 self.encoding = self.encoding.unsqueeze(0) def forward(self, x): &#34;&#34;&#34; 前向传播方法，将位置编码加到输入张量上 @param x: 输入张量，形状为 (batch_size, seq_len, d_model) @return: 添加了位置编码后的张量 &#34;&#34;&#34; # 在输入张量的第二维上加上位置编码 x = x + self.encoding[:, :x.size(1)] return x class MultiheadAttention(nn.Module): def __init__(self, d_model, n_heads): super(MultiheadAttention, self).__init__() self.d_model = d_model # 模型输入维度 self.n_heads = n_heads # 注意力头的数量 # 计算每个注意力头的维度 self.head_dim = d_model // n_heads # Query、Key和Value的线性变换 self.WQ = nn.Linear(d_model, d_model) self.WK = nn.Linear(d_model, d_model) self.WV = nn.Linear(d_model, d_model) # 注意力机制输出后的线性变换 self.fc_out = nn.Linear(d_model, d_model) def forward(self, query, key, value, mask=None): # mask在Encoder时是padding mask，在Decoder时是padding+自回归mask batch_size = query.size(0) # 对Query、Key和Value进行线性变换 # print(query.shape) Q = self.WQ(query) K = self.WK(key) V = self.WV(value) # print(Q.shape) # 为了进行多头注意力，对Q、K、V进行形状变换和转置 # TODO:这个地方，多头是把Embedding_dimension拆掉了？512变成了8个头和64？ # Embedding_dimension是一个单词或标记的编码维度。拆分和多头注意力机制的设计是为了让模型更好地捕捉输入序列的不同方面和关系。 Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) K = K.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) V = V.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # 计算缩放点积注意力得分 scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim) # 如果提供了掩码，应用掩码 if mask is not None: scores = scores.masked_fill(mask == 0, float(&#39;-1e20&#39;)) # 应用softmax获取注意力权重，dim 参数用于指定在哪个维度上进行 softmax scores = F.softmax(scores, dim=-1) # 应用注意力权重到Values上 attention = torch.matmul(scores, V) # 恢复原始形状,[batch_size, num_heads, seq_len, embedding_dim]-&gt;[batch_size, seq_len, num_heads, embedding_dim] # .contiguous() 是一个用于确保张量在内存中是连续的方法，切片和转置会造成矩阵在内存中不连续。 # .view(batch_size, -1, self.d_model): 这一步是对张量进行形状的变换。 # -1表示该维度的大小由其他维度决定，这里的目的是将num_heads和embedding_dim # 这两个维度合并成一个维度。最终得到的形状为[batch_size, seq_len, num_heads * embedding_dim]。 # 注意：注意力的多头输出仍然保持在同一个张量中，并没有进行显式的拼接操作。不是像 GoogleNet 那样进行拼接。 # 这样的设计是为了保持并行计算的效率，同时融合了不同头的信息。 attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model) # TODO:多头注意力后，有 fc 吗？ # 最终输出的线性变换 output = self.fc_out(attention) return output # 定义位置前馈神经网络层 class PositionwiseFeedforward(nn.Module): def __init__(self, d_model, d_ff): super(PositionwiseFeedforward, self).__init__() # 第一个全连接层，输入维度为d_model，输出维度为d_ff self.fc1 = nn.Linear(d_model, d_ff) # 第二个全连接层，输入维度为d_ff，输出维度为d_model self.fc2 = nn.Linear(d_ff, d_model) def forward(self, x): # TODO:x是一个torch.Size([32, 20, 512])的张量。 # 使用ReLU激活函数的第一个全连接层 x = F.relu(self.fc1(x)) # 第二个全连接层，无激活函数 x = self.fc2(x) return x # 定义Transformer编码器 class TransformerEncoder(nn.Module): def __init__(self, d_model, n_heads, n_layers, d_ff): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param n_layers: Transformer层的数量 @param d_ff: 位置前馈神经网络中间层的维度 &#34;&#34;&#34; super(TransformerEncoder, self).__init__() # 词嵌入层，输入词汇表大小，输出维度为d_model self.embedding = nn.Embedding(input_vocab_size, d_model) # 位置编码层 self.positional_encoding = PositionalEncoding(d_model) # 由多个Transformer编码器层组成的层列表 self.transformer_layers = nn.ModuleList( [TransformerEncoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)]) def forward(self, x, mask=None): # 输入序列经过词嵌入层 x = self.embedding(x) # 加上位置编码 x = self.positional_encoding(x) # 通过多个Transformer编码器层 for layer in self.transformer_layers: x = layer(x, mask) return x # 定义Transformer编码器层 class TransformerEncoderLayer(nn.Module): def __init__(self, d_model, n_heads, d_ff): super(TransformerEncoderLayer, self).__init__() # 多头自注意力层 self.self_attention = MultiheadAttention(d_model, n_heads) # 位置前馈神经网络层 self.feedforward = PositionwiseFeedforward(d_model, d_ff) # 第一个Layer Normalization层，用于多头自注意力层输出后的残差连接 self.norm1 = nn.LayerNorm(d_model) # 第二个Layer Normalization层，用于前馈神经网络层输出后的残差连接 self.norm2 = nn.LayerNorm(d_model) # Dropout层，用于增加模型的泛化能力 self.dropout = nn.Dropout(0.1) def forward(self, x, mask=None): # 多头自注意力层的前向传播，传入相同的Query、Key和Value（self-attention） self_attention_out = self.self_attention(x, x, x, mask=mask) # 残差连接和Layer Normalization x = x + self.dropout(self_attention_out) x = self.norm1(x) # 位置前馈神经网络层的前向传播 ff_out = self.feedforward(x) # 残差连接和Layer Normalization x = x + self.dropout(ff_out) x = self.norm2(x) return x class TransformerDecoderLayer(nn.Module): def __init__(self, d_model, n_heads, d_ff): &#34;&#34;&#34; 定义Transformer解码器层 @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param d_ff: 位置前馈神经网络中间层的维度 &#34;&#34;&#34; super(TransformerDecoderLayer, self).__init__() # 多头自注意力层，用于处理目标序列的内部关系 self.self_attention = MultiheadAttention(d_model, n_heads) # 多头注意力层，用于处理源序列到目标序列的关系 self.encoder_attention = MultiheadAttention(d_model, n_heads) # 位置前馈神经网络层 self.feedforward = PositionwiseFeedforward(d_model, d_ff) # 第一个Layer Normalization层，用于自注意力层输出后的残差连接 self.norm1 = nn.LayerNorm(d_model) # 第二个Layer Normalization层，用于源到目标注意力层输出后的残差连接 self.norm2 = nn.LayerNorm(d_model) # 第三个Layer Normalization层，用于前馈神经网络层输出后的残差连接 self.norm3 = nn.LayerNorm(d_model) # Dropout层，用于增加模型的泛化能力 self.dropout = nn.Dropout(0.1) def forward(self, x, encoder_output, src_mask=None, trg_mask=None): # 多头自注意力层的前向传播，传入相同的Query、Key和Value（self-attention），使用目标序列的mask self_attention_out = self.self_attention(x, x, x, mask=trg_mask) # 残差连接和Layer Normalization x = x + self.dropout(self_attention_out) x = self.norm1(x) # 多头注意力层的前向传播，传入Query为解码器的输出，Key和Value为编码器的输出，使用源序列的mask encoder_attention_out = self.encoder_attention(x, encoder_output, encoder_output, mask=src_mask) # 残差连接和Layer Normalization x = x + self.dropout(encoder_attention_out) x = self.norm2(x) # 位置前馈神经网络层的前向传播 ff_out = self.feedforward(x) # 残差连接和Layer Normalization x = x + self.dropout(ff_out) x = self.norm3(x) return x # 定义Transformer解码器 class TransformerDecoder(nn.Module): def __init__(self, d_model, n_heads, n_layers, d_ff, output_vocab_size): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param n_layers: Transformer层的数量 @param d_ff: 位置前馈神经网络中间层的维度 @param output_vocab_size: 输出词汇表的大小 &#34;&#34;&#34; super(TransformerDecoder, self).__init__() # 词嵌入层，输入词汇表大小，输出维度为d_model self.embedding = nn.Embedding(output_vocab_size, d_model) # 位置编码层 self.positional_encoding = PositionalEncoding(d_model) # 由多个Transformer解码器层组成的层列表 self.decoder_layers = nn.ModuleList([TransformerDecoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)]) # 最终输出层，将解码器的输出映射到词汇表大小 self.fc_out = nn.Linear(d_model, output_vocab_size) def forward(self, encoder_output, trg, src_mask=None, trg_mask=None): # 目标序列的词嵌入和位置编码 trg_emb = self.embedding(trg) trg_emb = self.positional_encoding(trg_emb) # 通过多个Transformer解码器层 for layer in self.decoder_layers: trg_emb = layer(trg_emb, encoder_output, src_mask, trg_mask) # 最终输出层，将解码器的输出映射到词汇表大小 output = self.fc_out(trg_emb) return output # 定义整体的Transformer模型 class Transformer(nn.Module): def __init__(self, d_model, n_heads, n_layers, d_ff, output_vocab_size): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param n_layers: Transformer层的数量 @param d_ff: 位置前馈神经网络中间层的维度 @param input_vocab_size: 输入词汇表的大小 @param output_vocab_size: 输出词汇表的大小 &#34;&#34;&#34; # TODO，这个地方没用到input_vocab_size super(Transformer, self).__init__() # Transformer编码器 self.encoder = TransformerEncoder(d_model, n_heads, n_layers, d_ff) # Transformer解码器 self.decoder = TransformerDecoder(d_model, n_heads, n_layers, d_ff, output_vocab_size) def forward(self, src, trg): # 源序列的padding mask，插入维度后：(batch_size, 1, 1, sequence_length) src_mask = (src != 0).unsqueeze(1).unsqueeze(2) # 目标序列的mask trg_mask = self.create_target_mask(trg) # 通过编码器得到编码器的输出 encoder_output = self.encoder(src, src_mask) # 通过解码器得到最终输出 output = self.decoder(encoder_output, trg, src_mask, trg_mask) return output # 创建目标序列的mask，包括填充部分和未来部分 def create_target_mask(self, target_data): # 创建目标序列的填充mask # (target_data != 0) 生成一个布尔张量，表示目标序列中非填充位置的元素 # .unsqueeze(1).unsqueeze(2) 在布尔张量的第一维和第二维上插入新的维度 # 形状变为 (batch_size, 1, 1, sequence_length) trg_pad_mask = (target_data != 0).unsqueeze(1).unsqueeze(2) # 获取目标序列的长度, (batch_size, sequence_length)取下角标1，即sequence_length=10 trg_len = target_data.size(1) # 创建目标序列的自回归（subsequent）mask，(sequence_length, sequence_length) # torch.tril(torch.ones(...)) 生成下三角矩阵，对角线及以下的元素为1，其余为0 # .bool() 将矩阵元素类型转换为布尔型 # 确保在计算自注意力时每个位置只能关注到当前位置及之前的位置 trg_subsequent_mask = torch.tril(torch.ones(trg_len, trg_len)).bool() # 将填充mask和自回归mask结合，取两者的逻辑与，得到最终的目标序列mask，(batch_size, 1, sequence_length, sequence_length) trg_mask = trg_pad_mask &amp; trg_subsequent_mask return trg_mask # 示例用法 # Transformer模型的参数配置 d_model = 512 # 模型的隐藏层维度 n_heads = 8 # 注意力头的数量 n_layers = 6 # Transformer层的数量 d_ff = 2048 # 位置前馈神经网络中间层的维度 input_vocab_size = 10000 # 输入词汇表的大小 output_vocab_size = 10000 # 输出词汇表的大小 # 数据批次配置 # 例如，在机器翻译任务中，src_seq_length 可以表示输入语言的句子长度，而 trg_seq_length 表示对应的目标语言的句子长度。 # 这两个值可以根据数据集的最大句子长度进行设置。 batch_size = 32 # 每个批次的样本数量 src_seq_length = 20 # 源序列的长度 trg_seq_length = 10 # 目标序列的长度 # 创建Transformer模型实例 transformer = Transformer(d_model, n_heads, n_layers, d_ff, output_vocab_size) # 随机生成输入序列和目标序列数据，范围0-output_vocab_size，(batch_size, src_seq_length)：这是生成的整数张量的形状 input_data = torch.randint(0, input_vocab_size, (batch_size, src_seq_length)) target_data = torch.randint(0, output_vocab_size, (batch_size, trg_seq_length)) # 将数据传入Transformer模型进行前向传播 output = transformer(input_data, target_data) # 打印输出张量的形状 print(output.shape) # from tensorboardX import SummaryWriter # with SummaryWriter(log_dir=&#39;&#39;) as sw: # 实例化 SummaryWriter ,可以自定义数据输出路径 # sw.add_graph(transformer, (input_data,target_data)) # 输出网络结构图 # sw.close() # 关闭 sw ">
<title>构建Transformer</title>

<link rel='canonical' href='https://UPPO8.github.io/Myblog/p/%E6%9E%84%E5%BB%BAtransformer/'>

<link rel="stylesheet" href="/Myblog/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css"><meta property='og:title' content="构建Transformer">
<meta property='og:description' content="Transformer学习记录，py3.8\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 import torch import torch.nn as nn import torch.nn.functional as F import math # 不带可训练权重的PositionalEncoding class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 &#34;&#34;&#34; super(PositionalEncoding, self).__init__() # 初始化位置编码矩阵，形状为(max_len=5000, d_model=1000)，其中的元素都是0。 self.encoding = torch.zeros(max_len, d_model) # 生成位置信息，形状为(max_len=5000, 1)，其中的元素是从0到max_len-1的整数。 position = torch.arange(0, max_len).unsqueeze(1).float() # # 计算分母的指数项，用于生成正弦和余弦函数的周期。 # # torch.arange(0, d_model, 2).float()：创建了一个从0到d_model-1的整数序列，步长为2 # div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) # # 利用正弦和余弦函数生成位置编码矩阵 # self.encoding[:, 0::2] = torch.sin(position * div_term) # self.encoding[:, 1::2] = torch.cos(position * div_term) # 利用正弦和余弦函数生成位置编码矩阵 div_term = torch.arange(0, d_model, 2).float() / d_model self.encoding[:, 0::2] = torch.sin(position / torch.pow(10000.0, div_term)) self.encoding[:, 1::2] = torch.cos(position / torch.pow(10000.0, div_term)) # 将位置编码矩阵扩展为三维 # self.encoding 是一个形状为 (max_len, d_model) 的二维张量，表示位置编码矩阵 # 通过 unsqueeze(0)，形状变为 (1, max_len, d_model) # 以便与输入嵌入的词向量(batch_size, sequence_length, embedding_dim)进行相加 self.encoding = self.encoding.unsqueeze(0) def forward(self, x): &#34;&#34;&#34; 前向传播方法，将位置编码加到输入张量上 @param x: 输入张量，形状为 (batch_size, seq_len, d_model) @return: 添加了位置编码后的张量 &#34;&#34;&#34; # 在输入张量的第二维上加上位置编码 x = x + self.encoding[:, :x.size(1)] return x class MultiheadAttention(nn.Module): def __init__(self, d_model, n_heads): super(MultiheadAttention, self).__init__() self.d_model = d_model # 模型输入维度 self.n_heads = n_heads # 注意力头的数量 # 计算每个注意力头的维度 self.head_dim = d_model // n_heads # Query、Key和Value的线性变换 self.WQ = nn.Linear(d_model, d_model) self.WK = nn.Linear(d_model, d_model) self.WV = nn.Linear(d_model, d_model) # 注意力机制输出后的线性变换 self.fc_out = nn.Linear(d_model, d_model) def forward(self, query, key, value, mask=None): # mask在Encoder时是padding mask，在Decoder时是padding+自回归mask batch_size = query.size(0) # 对Query、Key和Value进行线性变换 # print(query.shape) Q = self.WQ(query) K = self.WK(key) V = self.WV(value) # print(Q.shape) # 为了进行多头注意力，对Q、K、V进行形状变换和转置 # TODO:这个地方，多头是把Embedding_dimension拆掉了？512变成了8个头和64？ # Embedding_dimension是一个单词或标记的编码维度。拆分和多头注意力机制的设计是为了让模型更好地捕捉输入序列的不同方面和关系。 Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) K = K.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) V = V.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # 计算缩放点积注意力得分 scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim) # 如果提供了掩码，应用掩码 if mask is not None: scores = scores.masked_fill(mask == 0, float(&#39;-1e20&#39;)) # 应用softmax获取注意力权重，dim 参数用于指定在哪个维度上进行 softmax scores = F.softmax(scores, dim=-1) # 应用注意力权重到Values上 attention = torch.matmul(scores, V) # 恢复原始形状,[batch_size, num_heads, seq_len, embedding_dim]-&gt;[batch_size, seq_len, num_heads, embedding_dim] # .contiguous() 是一个用于确保张量在内存中是连续的方法，切片和转置会造成矩阵在内存中不连续。 # .view(batch_size, -1, self.d_model): 这一步是对张量进行形状的变换。 # -1表示该维度的大小由其他维度决定，这里的目的是将num_heads和embedding_dim # 这两个维度合并成一个维度。最终得到的形状为[batch_size, seq_len, num_heads * embedding_dim]。 # 注意：注意力的多头输出仍然保持在同一个张量中，并没有进行显式的拼接操作。不是像 GoogleNet 那样进行拼接。 # 这样的设计是为了保持并行计算的效率，同时融合了不同头的信息。 attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model) # TODO:多头注意力后，有 fc 吗？ # 最终输出的线性变换 output = self.fc_out(attention) return output # 定义位置前馈神经网络层 class PositionwiseFeedforward(nn.Module): def __init__(self, d_model, d_ff): super(PositionwiseFeedforward, self).__init__() # 第一个全连接层，输入维度为d_model，输出维度为d_ff self.fc1 = nn.Linear(d_model, d_ff) # 第二个全连接层，输入维度为d_ff，输出维度为d_model self.fc2 = nn.Linear(d_ff, d_model) def forward(self, x): # TODO:x是一个torch.Size([32, 20, 512])的张量。 # 使用ReLU激活函数的第一个全连接层 x = F.relu(self.fc1(x)) # 第二个全连接层，无激活函数 x = self.fc2(x) return x # 定义Transformer编码器 class TransformerEncoder(nn.Module): def __init__(self, d_model, n_heads, n_layers, d_ff): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param n_layers: Transformer层的数量 @param d_ff: 位置前馈神经网络中间层的维度 &#34;&#34;&#34; super(TransformerEncoder, self).__init__() # 词嵌入层，输入词汇表大小，输出维度为d_model self.embedding = nn.Embedding(input_vocab_size, d_model) # 位置编码层 self.positional_encoding = PositionalEncoding(d_model) # 由多个Transformer编码器层组成的层列表 self.transformer_layers = nn.ModuleList( [TransformerEncoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)]) def forward(self, x, mask=None): # 输入序列经过词嵌入层 x = self.embedding(x) # 加上位置编码 x = self.positional_encoding(x) # 通过多个Transformer编码器层 for layer in self.transformer_layers: x = layer(x, mask) return x # 定义Transformer编码器层 class TransformerEncoderLayer(nn.Module): def __init__(self, d_model, n_heads, d_ff): super(TransformerEncoderLayer, self).__init__() # 多头自注意力层 self.self_attention = MultiheadAttention(d_model, n_heads) # 位置前馈神经网络层 self.feedforward = PositionwiseFeedforward(d_model, d_ff) # 第一个Layer Normalization层，用于多头自注意力层输出后的残差连接 self.norm1 = nn.LayerNorm(d_model) # 第二个Layer Normalization层，用于前馈神经网络层输出后的残差连接 self.norm2 = nn.LayerNorm(d_model) # Dropout层，用于增加模型的泛化能力 self.dropout = nn.Dropout(0.1) def forward(self, x, mask=None): # 多头自注意力层的前向传播，传入相同的Query、Key和Value（self-attention） self_attention_out = self.self_attention(x, x, x, mask=mask) # 残差连接和Layer Normalization x = x + self.dropout(self_attention_out) x = self.norm1(x) # 位置前馈神经网络层的前向传播 ff_out = self.feedforward(x) # 残差连接和Layer Normalization x = x + self.dropout(ff_out) x = self.norm2(x) return x class TransformerDecoderLayer(nn.Module): def __init__(self, d_model, n_heads, d_ff): &#34;&#34;&#34; 定义Transformer解码器层 @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param d_ff: 位置前馈神经网络中间层的维度 &#34;&#34;&#34; super(TransformerDecoderLayer, self).__init__() # 多头自注意力层，用于处理目标序列的内部关系 self.self_attention = MultiheadAttention(d_model, n_heads) # 多头注意力层，用于处理源序列到目标序列的关系 self.encoder_attention = MultiheadAttention(d_model, n_heads) # 位置前馈神经网络层 self.feedforward = PositionwiseFeedforward(d_model, d_ff) # 第一个Layer Normalization层，用于自注意力层输出后的残差连接 self.norm1 = nn.LayerNorm(d_model) # 第二个Layer Normalization层，用于源到目标注意力层输出后的残差连接 self.norm2 = nn.LayerNorm(d_model) # 第三个Layer Normalization层，用于前馈神经网络层输出后的残差连接 self.norm3 = nn.LayerNorm(d_model) # Dropout层，用于增加模型的泛化能力 self.dropout = nn.Dropout(0.1) def forward(self, x, encoder_output, src_mask=None, trg_mask=None): # 多头自注意力层的前向传播，传入相同的Query、Key和Value（self-attention），使用目标序列的mask self_attention_out = self.self_attention(x, x, x, mask=trg_mask) # 残差连接和Layer Normalization x = x + self.dropout(self_attention_out) x = self.norm1(x) # 多头注意力层的前向传播，传入Query为解码器的输出，Key和Value为编码器的输出，使用源序列的mask encoder_attention_out = self.encoder_attention(x, encoder_output, encoder_output, mask=src_mask) # 残差连接和Layer Normalization x = x + self.dropout(encoder_attention_out) x = self.norm2(x) # 位置前馈神经网络层的前向传播 ff_out = self.feedforward(x) # 残差连接和Layer Normalization x = x + self.dropout(ff_out) x = self.norm3(x) return x # 定义Transformer解码器 class TransformerDecoder(nn.Module): def __init__(self, d_model, n_heads, n_layers, d_ff, output_vocab_size): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param n_layers: Transformer层的数量 @param d_ff: 位置前馈神经网络中间层的维度 @param output_vocab_size: 输出词汇表的大小 &#34;&#34;&#34; super(TransformerDecoder, self).__init__() # 词嵌入层，输入词汇表大小，输出维度为d_model self.embedding = nn.Embedding(output_vocab_size, d_model) # 位置编码层 self.positional_encoding = PositionalEncoding(d_model) # 由多个Transformer解码器层组成的层列表 self.decoder_layers = nn.ModuleList([TransformerDecoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)]) # 最终输出层，将解码器的输出映射到词汇表大小 self.fc_out = nn.Linear(d_model, output_vocab_size) def forward(self, encoder_output, trg, src_mask=None, trg_mask=None): # 目标序列的词嵌入和位置编码 trg_emb = self.embedding(trg) trg_emb = self.positional_encoding(trg_emb) # 通过多个Transformer解码器层 for layer in self.decoder_layers: trg_emb = layer(trg_emb, encoder_output, src_mask, trg_mask) # 最终输出层，将解码器的输出映射到词汇表大小 output = self.fc_out(trg_emb) return output # 定义整体的Transformer模型 class Transformer(nn.Module): def __init__(self, d_model, n_heads, n_layers, d_ff, output_vocab_size): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param n_layers: Transformer层的数量 @param d_ff: 位置前馈神经网络中间层的维度 @param input_vocab_size: 输入词汇表的大小 @param output_vocab_size: 输出词汇表的大小 &#34;&#34;&#34; # TODO，这个地方没用到input_vocab_size super(Transformer, self).__init__() # Transformer编码器 self.encoder = TransformerEncoder(d_model, n_heads, n_layers, d_ff) # Transformer解码器 self.decoder = TransformerDecoder(d_model, n_heads, n_layers, d_ff, output_vocab_size) def forward(self, src, trg): # 源序列的padding mask，插入维度后：(batch_size, 1, 1, sequence_length) src_mask = (src != 0).unsqueeze(1).unsqueeze(2) # 目标序列的mask trg_mask = self.create_target_mask(trg) # 通过编码器得到编码器的输出 encoder_output = self.encoder(src, src_mask) # 通过解码器得到最终输出 output = self.decoder(encoder_output, trg, src_mask, trg_mask) return output # 创建目标序列的mask，包括填充部分和未来部分 def create_target_mask(self, target_data): # 创建目标序列的填充mask # (target_data != 0) 生成一个布尔张量，表示目标序列中非填充位置的元素 # .unsqueeze(1).unsqueeze(2) 在布尔张量的第一维和第二维上插入新的维度 # 形状变为 (batch_size, 1, 1, sequence_length) trg_pad_mask = (target_data != 0).unsqueeze(1).unsqueeze(2) # 获取目标序列的长度, (batch_size, sequence_length)取下角标1，即sequence_length=10 trg_len = target_data.size(1) # 创建目标序列的自回归（subsequent）mask，(sequence_length, sequence_length) # torch.tril(torch.ones(...)) 生成下三角矩阵，对角线及以下的元素为1，其余为0 # .bool() 将矩阵元素类型转换为布尔型 # 确保在计算自注意力时每个位置只能关注到当前位置及之前的位置 trg_subsequent_mask = torch.tril(torch.ones(trg_len, trg_len)).bool() # 将填充mask和自回归mask结合，取两者的逻辑与，得到最终的目标序列mask，(batch_size, 1, sequence_length, sequence_length) trg_mask = trg_pad_mask &amp; trg_subsequent_mask return trg_mask # 示例用法 # Transformer模型的参数配置 d_model = 512 # 模型的隐藏层维度 n_heads = 8 # 注意力头的数量 n_layers = 6 # Transformer层的数量 d_ff = 2048 # 位置前馈神经网络中间层的维度 input_vocab_size = 10000 # 输入词汇表的大小 output_vocab_size = 10000 # 输出词汇表的大小 # 数据批次配置 # 例如，在机器翻译任务中，src_seq_length 可以表示输入语言的句子长度，而 trg_seq_length 表示对应的目标语言的句子长度。 # 这两个值可以根据数据集的最大句子长度进行设置。 batch_size = 32 # 每个批次的样本数量 src_seq_length = 20 # 源序列的长度 trg_seq_length = 10 # 目标序列的长度 # 创建Transformer模型实例 transformer = Transformer(d_model, n_heads, n_layers, d_ff, output_vocab_size) # 随机生成输入序列和目标序列数据，范围0-output_vocab_size，(batch_size, src_seq_length)：这是生成的整数张量的形状 input_data = torch.randint(0, input_vocab_size, (batch_size, src_seq_length)) target_data = torch.randint(0, output_vocab_size, (batch_size, trg_seq_length)) # 将数据传入Transformer模型进行前向传播 output = transformer(input_data, target_data) # 打印输出张量的形状 print(output.shape) # from tensorboardX import SummaryWriter # with SummaryWriter(log_dir=&#39;&#39;) as sw: # 实例化 SummaryWriter ,可以自定义数据输出路径 # sw.add_graph(transformer, (input_data,target_data)) # 输出网络结构图 # sw.close() # 关闭 sw ">
<meta property='og:url' content='https://UPPO8.github.io/Myblog/p/%E6%9E%84%E5%BB%BAtransformer/'>
<meta property='og:site_name' content='UPPO Blog'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2024-12-22T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2024-12-22T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="构建Transformer">
<meta name="twitter:description" content="Transformer学习记录，py3.8\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 import torch import torch.nn as nn import torch.nn.functional as F import math # 不带可训练权重的PositionalEncoding class PositionalEncoding(nn.Module): def __init__(self, d_model, max_len=5000): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 &#34;&#34;&#34; super(PositionalEncoding, self).__init__() # 初始化位置编码矩阵，形状为(max_len=5000, d_model=1000)，其中的元素都是0。 self.encoding = torch.zeros(max_len, d_model) # 生成位置信息，形状为(max_len=5000, 1)，其中的元素是从0到max_len-1的整数。 position = torch.arange(0, max_len).unsqueeze(1).float() # # 计算分母的指数项，用于生成正弦和余弦函数的周期。 # # torch.arange(0, d_model, 2).float()：创建了一个从0到d_model-1的整数序列，步长为2 # div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) # # 利用正弦和余弦函数生成位置编码矩阵 # self.encoding[:, 0::2] = torch.sin(position * div_term) # self.encoding[:, 1::2] = torch.cos(position * div_term) # 利用正弦和余弦函数生成位置编码矩阵 div_term = torch.arange(0, d_model, 2).float() / d_model self.encoding[:, 0::2] = torch.sin(position / torch.pow(10000.0, div_term)) self.encoding[:, 1::2] = torch.cos(position / torch.pow(10000.0, div_term)) # 将位置编码矩阵扩展为三维 # self.encoding 是一个形状为 (max_len, d_model) 的二维张量，表示位置编码矩阵 # 通过 unsqueeze(0)，形状变为 (1, max_len, d_model) # 以便与输入嵌入的词向量(batch_size, sequence_length, embedding_dim)进行相加 self.encoding = self.encoding.unsqueeze(0) def forward(self, x): &#34;&#34;&#34; 前向传播方法，将位置编码加到输入张量上 @param x: 输入张量，形状为 (batch_size, seq_len, d_model) @return: 添加了位置编码后的张量 &#34;&#34;&#34; # 在输入张量的第二维上加上位置编码 x = x + self.encoding[:, :x.size(1)] return x class MultiheadAttention(nn.Module): def __init__(self, d_model, n_heads): super(MultiheadAttention, self).__init__() self.d_model = d_model # 模型输入维度 self.n_heads = n_heads # 注意力头的数量 # 计算每个注意力头的维度 self.head_dim = d_model // n_heads # Query、Key和Value的线性变换 self.WQ = nn.Linear(d_model, d_model) self.WK = nn.Linear(d_model, d_model) self.WV = nn.Linear(d_model, d_model) # 注意力机制输出后的线性变换 self.fc_out = nn.Linear(d_model, d_model) def forward(self, query, key, value, mask=None): # mask在Encoder时是padding mask，在Decoder时是padding+自回归mask batch_size = query.size(0) # 对Query、Key和Value进行线性变换 # print(query.shape) Q = self.WQ(query) K = self.WK(key) V = self.WV(value) # print(Q.shape) # 为了进行多头注意力，对Q、K、V进行形状变换和转置 # TODO:这个地方，多头是把Embedding_dimension拆掉了？512变成了8个头和64？ # Embedding_dimension是一个单词或标记的编码维度。拆分和多头注意力机制的设计是为了让模型更好地捕捉输入序列的不同方面和关系。 Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) K = K.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) V = V.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # 计算缩放点积注意力得分 scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim) # 如果提供了掩码，应用掩码 if mask is not None: scores = scores.masked_fill(mask == 0, float(&#39;-1e20&#39;)) # 应用softmax获取注意力权重，dim 参数用于指定在哪个维度上进行 softmax scores = F.softmax(scores, dim=-1) # 应用注意力权重到Values上 attention = torch.matmul(scores, V) # 恢复原始形状,[batch_size, num_heads, seq_len, embedding_dim]-&gt;[batch_size, seq_len, num_heads, embedding_dim] # .contiguous() 是一个用于确保张量在内存中是连续的方法，切片和转置会造成矩阵在内存中不连续。 # .view(batch_size, -1, self.d_model): 这一步是对张量进行形状的变换。 # -1表示该维度的大小由其他维度决定，这里的目的是将num_heads和embedding_dim # 这两个维度合并成一个维度。最终得到的形状为[batch_size, seq_len, num_heads * embedding_dim]。 # 注意：注意力的多头输出仍然保持在同一个张量中，并没有进行显式的拼接操作。不是像 GoogleNet 那样进行拼接。 # 这样的设计是为了保持并行计算的效率，同时融合了不同头的信息。 attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model) # TODO:多头注意力后，有 fc 吗？ # 最终输出的线性变换 output = self.fc_out(attention) return output # 定义位置前馈神经网络层 class PositionwiseFeedforward(nn.Module): def __init__(self, d_model, d_ff): super(PositionwiseFeedforward, self).__init__() # 第一个全连接层，输入维度为d_model，输出维度为d_ff self.fc1 = nn.Linear(d_model, d_ff) # 第二个全连接层，输入维度为d_ff，输出维度为d_model self.fc2 = nn.Linear(d_ff, d_model) def forward(self, x): # TODO:x是一个torch.Size([32, 20, 512])的张量。 # 使用ReLU激活函数的第一个全连接层 x = F.relu(self.fc1(x)) # 第二个全连接层，无激活函数 x = self.fc2(x) return x # 定义Transformer编码器 class TransformerEncoder(nn.Module): def __init__(self, d_model, n_heads, n_layers, d_ff): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param n_layers: Transformer层的数量 @param d_ff: 位置前馈神经网络中间层的维度 &#34;&#34;&#34; super(TransformerEncoder, self).__init__() # 词嵌入层，输入词汇表大小，输出维度为d_model self.embedding = nn.Embedding(input_vocab_size, d_model) # 位置编码层 self.positional_encoding = PositionalEncoding(d_model) # 由多个Transformer编码器层组成的层列表 self.transformer_layers = nn.ModuleList( [TransformerEncoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)]) def forward(self, x, mask=None): # 输入序列经过词嵌入层 x = self.embedding(x) # 加上位置编码 x = self.positional_encoding(x) # 通过多个Transformer编码器层 for layer in self.transformer_layers: x = layer(x, mask) return x # 定义Transformer编码器层 class TransformerEncoderLayer(nn.Module): def __init__(self, d_model, n_heads, d_ff): super(TransformerEncoderLayer, self).__init__() # 多头自注意力层 self.self_attention = MultiheadAttention(d_model, n_heads) # 位置前馈神经网络层 self.feedforward = PositionwiseFeedforward(d_model, d_ff) # 第一个Layer Normalization层，用于多头自注意力层输出后的残差连接 self.norm1 = nn.LayerNorm(d_model) # 第二个Layer Normalization层，用于前馈神经网络层输出后的残差连接 self.norm2 = nn.LayerNorm(d_model) # Dropout层，用于增加模型的泛化能力 self.dropout = nn.Dropout(0.1) def forward(self, x, mask=None): # 多头自注意力层的前向传播，传入相同的Query、Key和Value（self-attention） self_attention_out = self.self_attention(x, x, x, mask=mask) # 残差连接和Layer Normalization x = x + self.dropout(self_attention_out) x = self.norm1(x) # 位置前馈神经网络层的前向传播 ff_out = self.feedforward(x) # 残差连接和Layer Normalization x = x + self.dropout(ff_out) x = self.norm2(x) return x class TransformerDecoderLayer(nn.Module): def __init__(self, d_model, n_heads, d_ff): &#34;&#34;&#34; 定义Transformer解码器层 @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param d_ff: 位置前馈神经网络中间层的维度 &#34;&#34;&#34; super(TransformerDecoderLayer, self).__init__() # 多头自注意力层，用于处理目标序列的内部关系 self.self_attention = MultiheadAttention(d_model, n_heads) # 多头注意力层，用于处理源序列到目标序列的关系 self.encoder_attention = MultiheadAttention(d_model, n_heads) # 位置前馈神经网络层 self.feedforward = PositionwiseFeedforward(d_model, d_ff) # 第一个Layer Normalization层，用于自注意力层输出后的残差连接 self.norm1 = nn.LayerNorm(d_model) # 第二个Layer Normalization层，用于源到目标注意力层输出后的残差连接 self.norm2 = nn.LayerNorm(d_model) # 第三个Layer Normalization层，用于前馈神经网络层输出后的残差连接 self.norm3 = nn.LayerNorm(d_model) # Dropout层，用于增加模型的泛化能力 self.dropout = nn.Dropout(0.1) def forward(self, x, encoder_output, src_mask=None, trg_mask=None): # 多头自注意力层的前向传播，传入相同的Query、Key和Value（self-attention），使用目标序列的mask self_attention_out = self.self_attention(x, x, x, mask=trg_mask) # 残差连接和Layer Normalization x = x + self.dropout(self_attention_out) x = self.norm1(x) # 多头注意力层的前向传播，传入Query为解码器的输出，Key和Value为编码器的输出，使用源序列的mask encoder_attention_out = self.encoder_attention(x, encoder_output, encoder_output, mask=src_mask) # 残差连接和Layer Normalization x = x + self.dropout(encoder_attention_out) x = self.norm2(x) # 位置前馈神经网络层的前向传播 ff_out = self.feedforward(x) # 残差连接和Layer Normalization x = x + self.dropout(ff_out) x = self.norm3(x) return x # 定义Transformer解码器 class TransformerDecoder(nn.Module): def __init__(self, d_model, n_heads, n_layers, d_ff, output_vocab_size): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param n_layers: Transformer层的数量 @param d_ff: 位置前馈神经网络中间层的维度 @param output_vocab_size: 输出词汇表的大小 &#34;&#34;&#34; super(TransformerDecoder, self).__init__() # 词嵌入层，输入词汇表大小，输出维度为d_model self.embedding = nn.Embedding(output_vocab_size, d_model) # 位置编码层 self.positional_encoding = PositionalEncoding(d_model) # 由多个Transformer解码器层组成的层列表 self.decoder_layers = nn.ModuleList([TransformerDecoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)]) # 最终输出层，将解码器的输出映射到词汇表大小 self.fc_out = nn.Linear(d_model, output_vocab_size) def forward(self, encoder_output, trg, src_mask=None, trg_mask=None): # 目标序列的词嵌入和位置编码 trg_emb = self.embedding(trg) trg_emb = self.positional_encoding(trg_emb) # 通过多个Transformer解码器层 for layer in self.decoder_layers: trg_emb = layer(trg_emb, encoder_output, src_mask, trg_mask) # 最终输出层，将解码器的输出映射到词汇表大小 output = self.fc_out(trg_emb) return output # 定义整体的Transformer模型 class Transformer(nn.Module): def __init__(self, d_model, n_heads, n_layers, d_ff, output_vocab_size): &#34;&#34;&#34; @param d_model: 模型的隐藏层维度 @param n_heads: 注意力头的数量 @param n_layers: Transformer层的数量 @param d_ff: 位置前馈神经网络中间层的维度 @param input_vocab_size: 输入词汇表的大小 @param output_vocab_size: 输出词汇表的大小 &#34;&#34;&#34; # TODO，这个地方没用到input_vocab_size super(Transformer, self).__init__() # Transformer编码器 self.encoder = TransformerEncoder(d_model, n_heads, n_layers, d_ff) # Transformer解码器 self.decoder = TransformerDecoder(d_model, n_heads, n_layers, d_ff, output_vocab_size) def forward(self, src, trg): # 源序列的padding mask，插入维度后：(batch_size, 1, 1, sequence_length) src_mask = (src != 0).unsqueeze(1).unsqueeze(2) # 目标序列的mask trg_mask = self.create_target_mask(trg) # 通过编码器得到编码器的输出 encoder_output = self.encoder(src, src_mask) # 通过解码器得到最终输出 output = self.decoder(encoder_output, trg, src_mask, trg_mask) return output # 创建目标序列的mask，包括填充部分和未来部分 def create_target_mask(self, target_data): # 创建目标序列的填充mask # (target_data != 0) 生成一个布尔张量，表示目标序列中非填充位置的元素 # .unsqueeze(1).unsqueeze(2) 在布尔张量的第一维和第二维上插入新的维度 # 形状变为 (batch_size, 1, 1, sequence_length) trg_pad_mask = (target_data != 0).unsqueeze(1).unsqueeze(2) # 获取目标序列的长度, (batch_size, sequence_length)取下角标1，即sequence_length=10 trg_len = target_data.size(1) # 创建目标序列的自回归（subsequent）mask，(sequence_length, sequence_length) # torch.tril(torch.ones(...)) 生成下三角矩阵，对角线及以下的元素为1，其余为0 # .bool() 将矩阵元素类型转换为布尔型 # 确保在计算自注意力时每个位置只能关注到当前位置及之前的位置 trg_subsequent_mask = torch.tril(torch.ones(trg_len, trg_len)).bool() # 将填充mask和自回归mask结合，取两者的逻辑与，得到最终的目标序列mask，(batch_size, 1, sequence_length, sequence_length) trg_mask = trg_pad_mask &amp; trg_subsequent_mask return trg_mask # 示例用法 # Transformer模型的参数配置 d_model = 512 # 模型的隐藏层维度 n_heads = 8 # 注意力头的数量 n_layers = 6 # Transformer层的数量 d_ff = 2048 # 位置前馈神经网络中间层的维度 input_vocab_size = 10000 # 输入词汇表的大小 output_vocab_size = 10000 # 输出词汇表的大小 # 数据批次配置 # 例如，在机器翻译任务中，src_seq_length 可以表示输入语言的句子长度，而 trg_seq_length 表示对应的目标语言的句子长度。 # 这两个值可以根据数据集的最大句子长度进行设置。 batch_size = 32 # 每个批次的样本数量 src_seq_length = 20 # 源序列的长度 trg_seq_length = 10 # 目标序列的长度 # 创建Transformer模型实例 transformer = Transformer(d_model, n_heads, n_layers, d_ff, output_vocab_size) # 随机生成输入序列和目标序列数据，范围0-output_vocab_size，(batch_size, src_seq_length)：这是生成的整数张量的形状 input_data = torch.randint(0, input_vocab_size, (batch_size, src_seq_length)) target_data = torch.randint(0, output_vocab_size, (batch_size, trg_seq_length)) # 将数据传入Transformer模型进行前向传播 output = transformer(input_data, target_data) # 打印输出张量的形状 print(output.shape) # from tensorboardX import SummaryWriter # with SummaryWriter(log_dir=&#39;&#39;) as sw: # 实例化 SummaryWriter ,可以自定义数据输出路径 # sw.add_graph(transformer, (input_data,target_data)) # 输出网络结构图 # sw.close() # 关闭 sw ">
    <link rel="shortcut icon" href="/Myblog/1828643.png" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/Myblog/">
                
                    
                    
                    
                        
                        <img src="/Myblog/img/100_hu_116fe1ebc7f4bd61.jpg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">😊</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/Myblog">UPPO Blog</a></h1>
            <h2 class="site-description">欢迎来到UPPO的博客</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://space.bilibili.com/402008155'
                        target="_blank"
                        title="BiliBili"
                        rel="me"
                    >
                        
                        
                            <svg class="icon" style="width: 1em;height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1385"><path d="M125.258667 395.2c-14.016 105.909333-6.4 234.293333-0.277334 337.504l0.021334 0.234667c3.413333 57.450667 28.405333 101.322667 72.277333 126.912 33.429333 19.498667 70.538667 25.109333 105.642667 28.266666 42.261333 3.808 120.736 7.861333 214.4 7.882667 93.685333-0.021333 172.16-4.085333 214.410666-7.893333 35.114667-3.146667 72.213333-8.757333 105.653334-28.266667 43.872-25.578667 68.864-69.461333 72.288-126.901333V732.8c6.133333-103.232 13.76-231.669333-0.277334-337.6-13.866667-104.864-91.274667-135.264-131.008-136.810667a3286.506667 3286.506667 0 0 0-106.026666-5.258666c9.813333-10.997333 25.898667-30.058667 37.525333-49.109334q17.888-29.301333-11.093333-53.717333-14.613333-12.448-28.405334-11.584-15.306667 0.96-29.578666 18.293333c-20.437333 24.832-49.973333 72.16-63.306667 94.005334a4644.352 4644.352 0 0 0-51.072-0.277334l-69.28 0.277334c-13.333333-21.845333-42.858667-69.173333-63.306667-94.005334q-14.282667-17.333333-29.578666-18.293333-13.781333-0.853333-28.394667 11.584-28.992 24.416-11.114667 53.717333c11.637333 19.050667 27.712 38.112 37.525334 49.109334-35.370667 1.173333-70.72 2.933333-106.026667 5.258666-39.722667 1.536-117.12 31.946667-130.997333 136.810667z" fill="#FFFFFF" p-id="1386"></path><path d="M124.981333 732.704c-6.122667-103.210667-13.738667-231.594667 0.277334-337.504 13.866667-104.864 91.274667-135.264 130.997333-136.810667 35.306667-2.325333 70.656-4.085333 106.026667-5.258666-9.813333-10.997333-25.888-30.058667-37.525334-49.109334q-17.888-29.301333 11.114667-53.717333 14.613333-12.437333 28.394667-11.584 15.296 0.96 29.578666 18.293333c20.448 24.832 49.973333 72.16 63.306667 94.005334l69.28-0.277334c17.28 0 34.293333 0.106667 51.072 0.277334 13.333333-21.845333 42.869333-69.173333 63.306667-94.005334q14.272-17.333333 29.578666-18.293333 13.781333-0.853333 28.405334 11.584 28.981333 24.416 11.093333 53.717333c-11.626667 19.050667-27.701333 38.112-37.514667 49.109334 36.928 1.258667 72.341333 3.008 106.026667 5.258666 39.722667 1.536 117.12 31.946667 130.997333 136.810667 14.026667 105.930667 6.4 234.368 0.288 337.6l-0.010666 0.138667c-3.413333 57.44-28.416 101.322667-72.288 126.912-33.44 19.498667-70.538667 25.109333-105.642667 28.266666-42.261333 3.797333-120.736 7.861333-214.410667 7.882667-93.674667-0.021333-172.16-4.074667-214.410666-7.893333-35.104-3.146667-72.213333-8.757333-105.642667-28.266667-43.872-25.578667-68.864-69.450667-72.277333-126.901333l-0.021334-0.234667z m650.4-401.141333l-1.386666-0.064c-74.474667-4.981333-157.76-7.509333-247.562667-7.509334h-18.208c-89.792 0-173.077333 2.528-247.552 7.52l-1.386667 0.064c-0.106667 0-12.181333 1.077333-24.928 8.746667-19.594667 11.754667-31.605333 33.429333-35.701333 64.384-13.12 99.104-5.706667 223.765333 0.245333 323.946667 3.829333 64.373333 43.498667 80.469333 110.741334 86.517333 14.037333 1.28 89.962667 7.562667 207.690666 7.584 117.738667-0.021333 193.664-6.314667 207.701334-7.584 67.242667-6.048 106.922667-22.144 110.741333-86.528 5.952-100.170667 13.354667-224.832 0.245333-323.936-4.096-30.965333-16.106667-52.629333-35.712-64.394667-12.746667-7.658667-24.821333-8.746667-24.928-8.746666zM249.941333 548.906667c7.488 15.146667 25.973333 21.418667 41.269334 13.973333 0.608-0.298667 61.568-29.706667 134.869333-51.946667 16.298667-4.906667 25.482667-21.994667 20.48-38.122666-4.992-16.138667-22.261333-25.194667-38.56-20.234667-77.92 23.637333-141.28 54.208-143.936 55.498667a30.368 30.368 0 0 0-14.112 40.832z m493.525334 13.973333c15.285333 7.402667 33.76 1.152 41.248-13.973333a30.368 30.368 0 0 0-14.101334-40.842667c-2.666667-1.290667-66.026667-31.861333-143.946666-55.488-16.288-4.949333-33.536 4.117333-38.506667 20.234667-4.992 16.106667 4.16 33.173333 20.448 38.122666 73.045333 22.154667 134.24 51.637333 134.858667 51.946667zM575.008 679.509333c51.658667 0 65.76-67.530667 66.325333-70.410666a14.645333 14.645333 0 0 0-11.637333-17.205334 14.773333 14.773333 0 0 0-17.397333 11.52c-0.106667 0.469333-10.133333 46.805333-37.290667 46.805334-29.098667 0-44.544-40.64-44.693333-41.002667a14.805333 14.805333 0 0 0-13.834667-9.674667h-0.074667c-6.186667 0-11.701333 3.797333-13.866666 9.525334-4.032 10.666667-20.330667 41.141333-41.653334 41.141333-20.394667 0-35.210667-35.498667-38.794666-47.936a14.826667 14.826667 0 0 0-18.208-9.952 14.634667 14.634667 0 0 0-10.282667 17.898667c2.016 7.093333 21.024 69.290667 67.306667 69.290666 25.834667 0 44.213333-18.133333 55.530666-34.496 11.562667 16.394667 30.656 34.496 58.570667 34.496z" fill="#00AEEC" p-id="1387"></path></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://github.com/ilovegta/ilovegta.github.io'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/Myblog/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Myblog/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>搜索</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Myblog/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>归档</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Myblog/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Myblog/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    

            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/Myblog/categories/%E6%97%A5%E5%B8%B8/" style="background-color: #2a9d8f; color: #fff;">
                日常
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/Myblog/p/%E6%9E%84%E5%BB%BAtransformer/">构建Transformer</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Dec 22, 2024</time>
            </div>
        

        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <p>Transformer学习记录，py3.8</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span><span class="lnt">258
</span><span class="lnt">259
</span><span class="lnt">260
</span><span class="lnt">261
</span><span class="lnt">262
</span><span class="lnt">263
</span><span class="lnt">264
</span><span class="lnt">265
</span><span class="lnt">266
</span><span class="lnt">267
</span><span class="lnt">268
</span><span class="lnt">269
</span><span class="lnt">270
</span><span class="lnt">271
</span><span class="lnt">272
</span><span class="lnt">273
</span><span class="lnt">274
</span><span class="lnt">275
</span><span class="lnt">276
</span><span class="lnt">277
</span><span class="lnt">278
</span><span class="lnt">279
</span><span class="lnt">280
</span><span class="lnt">281
</span><span class="lnt">282
</span><span class="lnt">283
</span><span class="lnt">284
</span><span class="lnt">285
</span><span class="lnt">286
</span><span class="lnt">287
</span><span class="lnt">288
</span><span class="lnt">289
</span><span class="lnt">290
</span><span class="lnt">291
</span><span class="lnt">292
</span><span class="lnt">293
</span><span class="lnt">294
</span><span class="lnt">295
</span><span class="lnt">296
</span><span class="lnt">297
</span><span class="lnt">298
</span><span class="lnt">299
</span><span class="lnt">300
</span><span class="lnt">301
</span><span class="lnt">302
</span><span class="lnt">303
</span><span class="lnt">304
</span><span class="lnt">305
</span><span class="lnt">306
</span><span class="lnt">307
</span><span class="lnt">308
</span><span class="lnt">309
</span><span class="lnt">310
</span><span class="lnt">311
</span><span class="lnt">312
</span><span class="lnt">313
</span><span class="lnt">314
</span><span class="lnt">315
</span><span class="lnt">316
</span><span class="lnt">317
</span><span class="lnt">318
</span><span class="lnt">319
</span><span class="lnt">320
</span><span class="lnt">321
</span><span class="lnt">322
</span><span class="lnt">323
</span><span class="lnt">324
</span><span class="lnt">325
</span><span class="lnt">326
</span><span class="lnt">327
</span><span class="lnt">328
</span><span class="lnt">329
</span><span class="lnt">330
</span><span class="lnt">331
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">math</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 不带可训练权重的PositionalEncoding</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param d_model: 模型的隐藏层维度
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 初始化位置编码矩阵，形状为(max_len=5000, d_model=1000)，其中的元素都是0。</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 生成位置信息，形状为(max_len=5000, 1)，其中的元素是从0到max_len-1的整数。</span>
</span></span><span class="line"><span class="cl">        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># # 计算分母的指数项，用于生成正弦和余弦函数的周期。</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># # torch.arange(0, d_model, 2).float()：创建了一个从0到d_model-1的整数序列，步长为2</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># # 利用正弦和余弦函数生成位置编码矩阵</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># self.encoding[:, 0::2] = torch.sin(position * div_term)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># self.encoding[:, 1::2] = torch.cos(position * div_term)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 利用正弦和余弦函数生成位置编码矩阵</span>
</span></span><span class="line"><span class="cl">        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">d_model</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">,</span> <span class="n">div_term</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">,</span> <span class="n">div_term</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 将位置编码矩阵扩展为三维</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># self.encoding 是一个形状为 (max_len, d_model) 的二维张量，表示位置编码矩阵</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 通过 unsqueeze(0)，形状变为 (1, max_len, d_model)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 以便与输入嵌入的词向量(batch_size, sequence_length, embedding_dim)进行相加</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        前向传播方法，将位置编码加到输入张量上
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param x: 输入张量，形状为 (batch_size, seq_len, d_model)
</span></span></span><span class="line"><span class="cl"><span class="s2">        @return: 添加了位置编码后的张量
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 在输入张量的第二维上加上位置编码</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">MultiheadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>  <span class="c1"># 模型输入维度</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>  <span class="c1"># 注意力头的数量</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算每个注意力头的维度</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Query、Key和Value的线性变换</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">WQ</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">WK</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">WV</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 注意力机制输出后的线性变换</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># mask在Encoder时是padding mask，在Decoder时是padding+自回归mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对Query、Key和Value进行线性变换</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># print(query.shape)</span>
</span></span><span class="line"><span class="cl">        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WQ</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WK</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WV</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># print(Q.shape)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 为了进行多头注意力，对Q、K、V进行形状变换和转置</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO:这个地方，多头是把Embedding_dimension拆掉了？512变成了8个头和64？</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Embedding_dimension是一个单词或标记的编码维度。拆分和多头注意力机制的设计是为了让模型更好地捕捉输入序列的不同方面和关系。</span>
</span></span><span class="line"><span class="cl">        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算缩放点积注意力得分</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 如果提供了掩码，应用掩码</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-1e20&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 应用softmax获取注意力权重，dim 参数用于指定在哪个维度上进行 softmax</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 应用注意力权重到Values上</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 恢复原始形状,[batch_size, num_heads, seq_len, embedding_dim]-&gt;[batch_size, seq_len, num_heads, embedding_dim]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># .contiguous() 是一个用于确保张量在内存中是连续的方法，切片和转置会造成矩阵在内存中不连续。</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># .view(batch_size, -1, self.d_model): 这一步是对张量进行形状的变换。</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># -1表示该维度的大小由其他维度决定，这里的目的是将num_heads和embedding_dim</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这两个维度合并成一个维度。最终得到的形状为[batch_size, seq_len, num_heads * embedding_dim]。</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 注意：注意力的多头输出仍然保持在同一个张量中，并没有进行显式的拼接操作。不是像 GoogleNet 那样进行拼接。</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这样的设计是为了保持并行计算的效率，同时融合了不同头的信息。</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO:多头注意力后，有 fc 吗？</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 最终输出的线性变换</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_out</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 定义位置前馈神经网络层</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PositionwiseFeedforward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">PositionwiseFeedforward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 第一个全连接层，输入维度为d_model，输出维度为d_ff</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 第二个全连接层，输入维度为d_ff，输出维度为d_model</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO:x是一个torch.Size([32, 20, 512])的张量。</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 使用ReLU激活函数的第一个全连接层</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 第二个全连接层，无激活函数</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 定义Transformer编码器</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param d_model: 模型的隐藏层维度
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param n_heads: 注意力头的数量
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param n_layers: Transformer层的数量
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param d_ff: 位置前馈神经网络中间层的维度
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 词嵌入层，输入词汇表大小，输出维度为d_model</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 位置编码层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 由多个Transformer编码器层组成的层列表</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">transformer_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span><span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 输入序列经过词嵌入层</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 加上位置编码</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 通过多个Transformer编码器层</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 定义Transformer编码器层</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TransformerEncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 多头自注意力层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 位置前馈神经网络层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span> <span class="o">=</span> <span class="n">PositionwiseFeedforward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 第一个Layer Normalization层，用于多头自注意力层输出后的残差连接</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 第二个Layer Normalization层，用于前馈神经网络层输出后的残差连接</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Dropout层，用于增加模型的泛化能力</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 多头自注意力层的前向传播，传入相同的Query、Key和Value（self-attention）</span>
</span></span><span class="line"><span class="cl">        <span class="n">self_attention_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 残差连接和Layer Normalization</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">self_attention_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 位置前馈神经网络层的前向传播</span>
</span></span><span class="line"><span class="cl">        <span class="n">ff_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 残差连接和Layer Normalization</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TransformerDecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        定义Transformer解码器层
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param d_model: 模型的隐藏层维度
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param n_heads: 注意力头的数量
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param d_ff: 位置前馈神经网络中间层的维度
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerDecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 多头自注意力层，用于处理目标序列的内部关系</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 多头注意力层，用于处理源序列到目标序列的关系</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attention</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 位置前馈神经网络层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span> <span class="o">=</span> <span class="n">PositionwiseFeedforward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 第一个Layer Normalization层，用于自注意力层输出后的残差连接</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 第二个Layer Normalization层，用于源到目标注意力层输出后的残差连接</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 第三个Layer Normalization层，用于前馈神经网络层输出后的残差连接</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Dropout层，用于增加模型的泛化能力</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trg_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 多头自注意力层的前向传播，传入相同的Query、Key和Value（self-attention），使用目标序列的mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">self_attention_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">trg_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 残差连接和Layer Normalization</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">self_attention_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 多头注意力层的前向传播，传入Query为解码器的输出，Key和Value为编码器的输出，使用源序列的mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_attention_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">src_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 残差连接和Layer Normalization</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">encoder_attention_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 位置前馈神经网络层的前向传播</span>
</span></span><span class="line"><span class="cl">        <span class="n">ff_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedforward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 残差连接和Layer Normalization</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 定义Transformer解码器</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">output_vocab_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param d_model: 模型的隐藏层维度
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param n_heads: 注意力头的数量
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param n_layers: Transformer层的数量
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param d_ff: 位置前馈神经网络中间层的维度
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param output_vocab_size: 输出词汇表的大小
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 词嵌入层，输入词汇表大小，输出维度为d_model</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">output_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 位置编码层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 由多个Transformer解码器层组成的层列表</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">TransformerDecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 最终输出层，将解码器的输出映射到词汇表大小</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">output_vocab_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">trg</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trg_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 目标序列的词嵌入和位置编码</span>
</span></span><span class="line"><span class="cl">        <span class="n">trg_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">trg_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="n">trg_emb</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 通过多个Transformer解码器层</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">trg_emb</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">trg_emb</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 最终输出层，将解码器的输出映射到词汇表大小</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_out</span><span class="p">(</span><span class="n">trg_emb</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 定义整体的Transformer模型</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">output_vocab_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param d_model: 模型的隐藏层维度
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param n_heads: 注意力头的数量
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param n_layers: Transformer层的数量
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param d_ff: 位置前馈神经网络中间层的维度
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param input_vocab_size: 输入词汇表的大小
</span></span></span><span class="line"><span class="cl"><span class="s2">        @param output_vocab_size: 输出词汇表的大小
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO，这个地方没用到input_vocab_size</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Transformer编码器</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Transformer解码器</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TransformerDecoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">output_vocab_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">trg</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 源序列的padding mask，插入维度后：(batch_size, 1, 1, sequence_length)</span>
</span></span><span class="line"><span class="cl">        <span class="n">src_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">src</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 目标序列的mask</span>
</span></span><span class="line"><span class="cl">        <span class="n">trg_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_target_mask</span><span class="p">(</span><span class="n">trg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 通过编码器得到编码器的输出</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 通过解码器得到最终输出</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">,</span> <span class="n">trg</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">trg_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 创建目标序列的mask，包括填充部分和未来部分</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">create_target_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 创建目标序列的填充mask</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (target_data != 0) 生成一个布尔张量，表示目标序列中非填充位置的元素</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># .unsqueeze(1).unsqueeze(2) 在布尔张量的第一维和第二维上插入新的维度</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 形状变为 (batch_size, 1, 1, sequence_length)</span>
</span></span><span class="line"><span class="cl">        <span class="n">trg_pad_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">target_data</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 获取目标序列的长度, (batch_size, sequence_length)取下角标1，即sequence_length=10</span>
</span></span><span class="line"><span class="cl">        <span class="n">trg_len</span> <span class="o">=</span> <span class="n">target_data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 创建目标序列的自回归（subsequent）mask，(sequence_length, sequence_length)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># torch.tril(torch.ones(...)) 生成下三角矩阵，对角线及以下的元素为1，其余为0</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># .bool() 将矩阵元素类型转换为布尔型</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 确保在计算自注意力时每个位置只能关注到当前位置及之前的位置</span>
</span></span><span class="line"><span class="cl">        <span class="n">trg_subsequent_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">trg_len</span><span class="p">,</span> <span class="n">trg_len</span><span class="p">))</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 将填充mask和自回归mask结合，取两者的逻辑与，得到最终的目标序列mask，(batch_size, 1, sequence_length, sequence_length)</span>
</span></span><span class="line"><span class="cl">        <span class="n">trg_mask</span> <span class="o">=</span> <span class="n">trg_pad_mask</span> <span class="o">&amp;</span> <span class="n">trg_subsequent_mask</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">trg_mask</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 示例用法</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Transformer模型的参数配置</span>
</span></span><span class="line"><span class="cl"><span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>          <span class="c1"># 模型的隐藏层维度</span>
</span></span><span class="line"><span class="cl"><span class="n">n_heads</span> <span class="o">=</span> <span class="mi">8</span>            <span class="c1"># 注意力头的数量</span>
</span></span><span class="line"><span class="cl"><span class="n">n_layers</span> <span class="o">=</span> <span class="mi">6</span>           <span class="c1"># Transformer层的数量</span>
</span></span><span class="line"><span class="cl"><span class="n">d_ff</span> <span class="o">=</span> <span class="mi">2048</span>            <span class="c1"># 位置前馈神经网络中间层的维度</span>
</span></span><span class="line"><span class="cl"><span class="n">input_vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>   <span class="c1"># 输入词汇表的大小</span>
</span></span><span class="line"><span class="cl"><span class="n">output_vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># 输出词汇表的大小</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 数据批次配置</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 例如，在机器翻译任务中，src_seq_length 可以表示输入语言的句子长度，而 trg_seq_length 表示对应的目标语言的句子长度。</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 这两个值可以根据数据集的最大句子长度进行设置。</span>
</span></span><span class="line"><span class="cl"><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>        <span class="c1"># 每个批次的样本数量</span>
</span></span><span class="line"><span class="cl"><span class="n">src_seq_length</span> <span class="o">=</span> <span class="mi">20</span>    <span class="c1"># 源序列的长度</span>
</span></span><span class="line"><span class="cl"><span class="n">trg_seq_length</span> <span class="o">=</span> <span class="mi">10</span>    <span class="c1"># 目标序列的长度</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建Transformer模型实例</span>
</span></span><span class="line"><span class="cl"><span class="n">transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">output_vocab_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 随机生成输入序列和目标序列数据，范围0-output_vocab_size，(batch_size, src_seq_length)：这是生成的整数张量的形状</span>
</span></span><span class="line"><span class="cl"><span class="n">input_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">src_seq_length</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">target_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">trg_seq_length</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 将数据传入Transformer模型进行前向传播</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 打印输出张量的形状</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># from tensorboardX import SummaryWriter</span>
</span></span><span class="line"><span class="cl"><span class="c1"># with SummaryWriter(log_dir=&#39;&#39;) as sw:  # 实例化 SummaryWriter ,可以自定义数据输出路径</span>
</span></span><span class="line"><span class="cl"><span class="c1">#     sw.add_graph(transformer, (input_data,target_data))  # 输出网络结构图</span>
</span></span><span class="line"><span class="cl"><span class="c1">#     sw.close()  # 关闭  sw</span>
</span></span></code></pre></td></tr></table>
</div>
</div>
</section>


    <footer class="article-footer">
    

    </footer>


    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">相关文章</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/Myblog/p/%E4%BD%BF%E7%94%A8python%E7%88%AC%E5%8F%96%E7%99%BE%E5%BA%A6%E5%9B%BE%E7%89%87/">
        
        

        <div class="article-details">
            <h2 class="article-title">使用Python爬取百度图片</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/Myblog/p/%E7%BD%91%E9%A1%B5%E5%B5%8C%E5%85%A5%E5%BC%8F%E9%9F%B3%E4%B9%90%E6%92%AD%E6%94%BE%E5%99%A8/">
        
        

        <div class="article-details">
            <h2 class="article-title">网页嵌入式音乐播放器</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2024 - 
        
        2025 UPPO
    </section>
    
    <section class="powerby">
        使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.27.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/Myblog/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

<script>
    
    const cssPath = "https://UPPO8.github.io/Myblog/waifu/waifu.css"
    const tipsJsonPath = "https://UPPO8.github.io/Myblog/waifu/waifu-tips.json"
    
    const live2d_path = "https://fastly.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/";

    
    function loadExternalResource(url, type) {
        return new Promise((resolve, reject) => {
            let tag;
            if (type === "css") {
                tag = document.createElement("link");
                tag.rel = "stylesheet";
                tag.href = url;
            }
            else if (type === "js") {
                tag = document.createElement("script");
                tag.src = url;
            }
            if (tag) {
                tag.onload = () => resolve(url);
                tag.onerror = () => reject(url);
                document.head.appendChild(tag);
            }
        });
    }

    
    if (screen.width >= 768) {
        Promise.all([
        loadExternalResource(cssPath, "css"),
            loadExternalResource(live2d_path + "live2d.min.js", "js"),
            loadExternalResource(live2d_path + "waifu-tips.js", "js")
        ]).then(() => {
            initWidget({
                waifuPath: tipsJsonPath,
                cdnPath: "https://fastly.jsdelivr.net/gh/fghrsh/live2d_api/",
                tools: ["hitokoto", "asteroids", "switch-model", "switch-texture", "photo", "info", "quit"]
            });
            
            initWaifuMouseEvent();
        });
    }
    function initWaifuMouseEvent() {
        const waifu = document.getElementById("waifu");
        let isDown = false;
        let waifuLeft;
        let mouseLeft;
        let waifuTop;
        let mouseTop;
        
        waifu.onmousedown = function (e) {
            isDown = true;
            
            waifuLeft = waifu.offsetLeft;
            mouseLeft = e.clientX;
            
            waifuTop = waifu.offsetTop;
            mouseTop = e.clientY;
        }
        
        window.onmousemove = function (e) {
            if (!isDown) {
                return;
            }
            
            let currentLeft = waifuLeft + (e.clientX - mouseLeft);
            if (currentLeft < 0) {
                currentLeft = 0;
            } else if (currentLeft > window.innerWidth - 300) {
                currentLeft = window.innerWidth - 300;
            }
            waifu.style.left = currentLeft  + "px";
            
            
            
            
            
            
            
            
            
            
        }
        
        window.onmouseup = function (e) {
            isDown = false;
        }
    }
</script>

    </body>
</html>
